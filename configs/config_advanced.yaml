# Data Config
dataset:
  root_dirs:
    - "./data/Augmented_Data_Zip"
  json_path: "./data/Augmented_Data_Zip/dataset_aug_metadata.json"
  batch_size: 32
  num_workers: 12
  val_split: 0.1 # 10% Validation
  test_split: 0.1 # 10% Hold-out Test
  augment_online: true
  
# -------------------------------------------------------------------------
# Student Model (EfficientNet-B0 + High-Freq CBAM)
# -------------------------------------------------------------------------
backbone: 
  cnn_model: "efficientnet_b0"
  vit_model: null 
  pretrained: true
  use_cbam: "every_2_blocks" 
  dropout: 0.3

# -------------------------------------------------------------------------
# Phase 1: SupCon (Pre-training)
# -------------------------------------------------------------------------
supcon:
  enabled: true
  epochs: 50
  batch_size: 16
  lr: 1.0e-3
  steps: 482000 # ~50 Epochs * 9625 steps/epoch
  use_sam: false # Optimization: Disable SAM
  snapshot_path: "./snapshots_advanced/supcon_final.pth" # Save here
  use_amp: true # Optimization: Re-enable AMP
  rho: 0.05 # Reset rho (unused without SAM)
  resume_from: "./snapshots_advanced/supcon_final_best.pth" # Resume from last best checkpoint
  early_stopping_patience: 100 # Optimization: Increase to 100 to prevent premature stopping

# -------------------------------------------------------------------------
# Phase 2: ArcFace (ULMFiT Fine-Tuning)
# -------------------------------------------------------------------------
arcface:
  enabled: true
  epochs: 20
  lr: 2.0e-5 # Optimization: Reduced from 1e-4 to 2e-5 for stability with Batch 16
  snapshot_dir: "./snapshots_advanced"
  
  # ULMFiT Strategy
  use_ulmfit: true
  gradual_unfreezing: true
  unfreeze_epoch: 2 # Epoch 1 Head Only, Epoch 2+ Full (Optimization: 1 epoch is enough for 600k data)
  discriminative_lr_decay: 2.6
  
  use_sam: false # Optimization: Disable SAM to save VRAM (Doubles memory usage)
  use_amp: true  # Optimization: Re-enable AMP (Stable without SAM)
  val_limit_batches: 10 # Check only 10 batches every 10 steps
  early_stopping_patience: 100 # Optimization: Increased to 100 to prevent premature stopping (gives ~1000 steps buffer)

# -------------------------------------------------------------------------
# Phase 3: Distillation (DISABLED)
# -------------------------------------------------------------------------
distill:
  enabled: false
  
embedding_size: 512
num_classes: 4
