project_name: "Garbage_Classification_Edge_v1"
experiment_name: "Distill_ResNetSwin_to_MobileNet"

# Data Config (Shared)
# Data Config (Shared)
dataset:
  root_dirs:
    - "d:/Done,Toreview/Image Classification/data"
  json_path: "d:/Done,Toreview/Image Classification/data/Dataset_Final_Aug/dataset_aug_metadata.json"
  batch_size: 128 # CLOUD MODE: Large batch for stable gradients
  num_workers: 8  # Use more cores
  val_split: 0.1
  test_split: 0.1
  augment_online: false # CLOUD MODE: User has pre-augmented data, disable online augs

# -------------------------------------------------------------------------
# Phase 1 & 2: The TEACHER (Big "MAX POWER" Model)
# -------------------------------------------------------------------------
backbone: 
  cnn_model: "resnet50"
  vit_model: "swin_base_patch4_window7_224" # CLOUD MODE: Enable Swin Transformer (Hybrid)
  pretrained: true
embedding_size: 512
num_classes: 4

supcon:
  enabled: true
  epochs: 100 
  batch_size: 128 # CLOUD MODE
  num_views: 2
  lr: 1.0e-4 # Higher LR for huge batch size
  steps: 500
  use_sam: true       # CLOUD MODE: Enable SAM
  use_lookahead: true # CLOUD MODE: Enable Lookahead
  early_stopping_patience: 50 # Increased to 50

arcface:
  enabled: true
  epochs: 50
  lr: 1.0e-4 
  snapshot_dir: "./snapshots_teacher"
  early_stopping_patience: 50 # Increased to 50
  use_amp: true 
  use_curricularface: false # AdaFace fits garbage best
  ema_decay: 0.9995   # CLOUD MODE: Enable EMA for smoothness
  use_sam: true       # CLOUD MODE: Enable SAM
  use_lookahead: true # CLOUD MODE: Enable Lookahead

# -------------------------------------------------------------------------
# Phase 3: The STUDENT (Tiny Edge Model)
# The pipeline will load the BEST Teacher from Phase 2, and train THIS config.
# -------------------------------------------------------------------------
distill:
  enabled: true
  epochs: 30
  distill_weight: 0.5 # 50% learn from Teacher, 50% learn from Hard Labels
  mix_method: "mixup"
  use_manifold_mixup: false
  temperature: 3.0    # Softer probability distribution for better transfer
  
  # Path to the Teacher we just trained (Pipeline defaults to this, but explicit is good)
  teacher_backbone_path: "./snapshots_teacher/best_model.pth"
  # We don't necessarily need the teacher head if using feature distillation, but for logits we do.
  # The code relies on finding the matching head for the backbone.
  
  # DEFINITION OF THE STUDENT (Raspberry Pi Friendly!)
  backbone:
    cnn_model: "mobilenetv3_large_100" # fast, lightweight
    vit_model: null                    # No transformer for edge speed
    pretrained: true
    fusion_dim: 1280                   # MobileNet output dim (auto-handled usually, but good to note)
  
  log_csv: "./logs/distill_edge_metrics.csv"
  snapshot_dir: "./snapshots_edge_student"
  early_stopping_patience: 50 # Increased to 50
  use_sam: true         # CLOUD MODE: Enable SAM
  use_lookahead: true   # CLOUD MODE: Enable Lookahead
  ema_decay: 0.9995     # CLOUD MODE: Enable EMA
